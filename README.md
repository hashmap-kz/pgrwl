**XXX** is a fully functional clone of PostgreSQL’s `pg_receivewal`, written in Go.

The project serves as a research platform to explore streaming WAL archiving with a target of **RPO=0** during recovery.

The utility replicates all key features of `pg_receivewal`, including automatic reconnection on connection loss, streaming
into partial files, and extensive error checking.

---

## Requirements

Make sure the following PostgreSQL environment variables are **explicitly set** before running the utility:

- `PGHOST` – PostgreSQL host
- `PGPORT` – PostgreSQL port
- `PGUSER` – Replication user
- `PGPASSWORD` – Replication password

These are used to establish a replication connection to PostgreSQL.

---

## Usage

```bash
XXX [OPTIONS]
```

### Required Flags

| Flag                | Description                                                        |
|---------------------|--------------------------------------------------------------------|
| `-D`, `--directory` | Directory to store WAL segments (it will be created automatically) |
| `-S`, `--slot`      | Replication slot name to use (a slot will be create automatically) |

### Optional Flags

| Flag               | Description                                                                |
|--------------------|----------------------------------------------------------------------------|
| `-n`, `--no-loop`  | Do not retry connection if it is lost                                      |
| `--log-level`      | Logging level: `trace`, `debug`, `info`, `warn`, `error` (default: `info`) |
| `--log-add-source` | Include source file and line number in log output (default: `false`)       |

---

## Notes on fsync (since the utility works only in synchronous mode):

* After each WAL segment is written, an fsync is performed on the currently open WAL file to ensure durability.
* An fsync is also triggered when a WAL segment is completed and the corresponding `*.partial` file is renamed to its final
  form.
* An fsync is triggered when we receive a keepalive msg from then server with 'reply_requested' option set.
* Additionally, fsync is called whenever an error occurs during the receive-copy loop.

---

## Notes on integration tests:

Here and example of a 'golden' fundamental test.
It verifies that we can restore to the latest committed transaction after an abrupt system crash.
It also checks that the WAL files generated are byte-for-byte identical to those generated by `pg_receivewal`.

### Steps:

* Initialize and start a PostgreSQL cluster
* Run WAL receivers (`XXX` and `pg_receivewal`)
* Create a base backup
* Create a table, and insert the current timestamp every second (in the background)
* Run pgbench to populate the database with 1 million rows
* Generate additional data (~512 MiB)
* Concurrently create 100 tables with 10000 rows each.
* Terminate the insert-script job
* Run pg_dumpall and save the output as plain SQL
* Terminate all PostgreSQL processes and delete the `PGDATA` directory (termination is force and abnormal)
* Restore `PGDATA` from the base backup, add recovery.signal, and configure restore_command
* Rename all `*.partial` WAL files in the WAL archive directories
* Start the PostgreSQL cluster (cluster should recover to the latest committed transaction)
* Run pg_dumpall after the cluster is ready
* Diff the pg_dumpall results (before and after)
* Check the insert-script logs and verify that the table contains the last inserted row
* Compare WAL directories (filenames and contents must match 100%)
* Clean up WAL directories and rerun the WAL archivers on a new timeline (cleanup is necessary since we run receivers
  with --no-loop option)
* Compare the WAL directories again

---

## Design notes

This utility follows a **single-responsibility** principle.

Is designed to use the local filesystem exclusively. This is a deliberate choice, because—as mentioned
earlier—we must rely on fsync after each message is written to disk.
This ensures that `*.partial` files always contain fully valid WAL segments, making them safe to use during the restore
phase (after simply removing the `*.partial` suffix).

I'm considering adding support for compression and encryption as optional features for completed WAL files.
However, streaming `*.partial` files to any location other than the local filesystem can introduce numerous unpredictable
issues.

In short: PostgreSQL waits for the replica to confirm commits, so we cannot afford to depend on external systems in such
critical paths.

This utility focuses on doing one thing well: robustly streaming WAL segments.
In my strong opinion, combining this core responsibility with other concerns—such as retention policies, remote uploads,
or additional processing—risks introducing failure modes.
If any of those subsystems were to crash, it could bring down the main WAL receiving loop, which is precisely the kind
of failure we aim to avoid.

Handling retention, remote uploads, and other lifecycle operations should be the responsibility of **separate tools** that
consume the WAL files archived by this utility.
This is the same design philosophy used by `pg_receivewal` and other official PostgreSQL tools.

---

## Notes on archive_command and archive_timeout

There’s a significant difference between using `archive_command` and archiving WAL files via the streaming replication
protocol.

The `archive_command` is triggered only after a WAL file is fully completed—typically when it reaches 16 MiB (the default
segment size).
This means that in a crash scenario, you could lose up to 16 MiB of data.

You can mitigate this by setting a lower `archive_timeout` (e.g., 1 minute), but even then, in a worst-case scenario, you
risk losing up to 1 minute of data.
Also, it’s important to note that PostgreSQL preallocates WAL files to the configured `wal_segment_size`, so they are
created with full size regardless of how much data has been written.

In contrast, streaming WAL archiving—when used with replication slots and the `synchronous_standby_names`
parameter—ensures that the system can be restored to the latest committed transaction.
This approach provides true zero data loss (**RPO=0**), making it ideal for high-durability requirements.













